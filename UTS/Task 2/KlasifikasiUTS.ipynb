{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bfacca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest, f_classif, chi2\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.stats import f_oneway\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be11128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv('KlasifikasiUTS.csv')\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c147b898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Encoding - Label Encoding for categorical columns\n",
    "cat_cols = df.select_dtypes(include='object').columns\n",
    "le = LabelEncoder()\n",
    "for col in cat_cols:\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# Feature Selection - Variance Threshold\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "selector.fit(df.drop(columns=['Class']))\n",
    "df.drop(columns=df.drop(columns=['Class']).columns[~selector.get_support()], inplace=True)\n",
    "\n",
    "# Feature Selection - Remove duplicate features\n",
    "duplicated_features = []\n",
    "cols = df.drop(columns=['Class']).columns\n",
    "for i in range(len(cols)):\n",
    "    for j in range(i + 1, len(cols)):\n",
    "        if np.allclose(df[cols[i]].values, df[cols[j]].values):\n",
    "            duplicated_features.append(cols[j])\n",
    "df.drop(columns=np.unique(duplicated_features), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d40d73b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "corr_matrix = df.corr()\n",
    "high_corr_var = set()\n",
    "threshold = 0.95\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "            colname = corr_matrix.columns[i]\n",
    "            high_corr_var.add(colname)\n",
    "df.drop(columns=high_corr_var, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c394e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "X = df.drop(columns=['Class'])\n",
    "y = df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "385a3148",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m mi_series = pd.Series(mi, index=X.columns).sort_values(ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Feature Selection - Chi-Square\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m X_chi2 = \u001b[43mSelectKBest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchi2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m chi2_features = X.columns[X_chi2.get_support()]\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Feature Selection - ANOVA F-test\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:569\u001b[39m, in \u001b[36m_BaseFilter.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    564\u001b[39m     X, y = validate_data(\n\u001b[32m    565\u001b[39m         \u001b[38;5;28mself\u001b[39m, X, y, accept_sparse=[\u001b[33m\"\u001b[39m\u001b[33mcsr\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcsc\u001b[39m\u001b[33m\"\u001b[39m], multi_output=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    566\u001b[39m     )\n\u001b[32m    568\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params(X, y)\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m score_func_ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscore_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(score_func_ret, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    571\u001b[39m     \u001b[38;5;28mself\u001b[39m.scores_, \u001b[38;5;28mself\u001b[39m.pvalues_ = score_func_ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:189\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m global_skip_validation = get_config()[\u001b[33m\"\u001b[39m\u001b[33mskip_parameter_validation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m func_sig = signature(func)\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:268\u001b[39m, in \u001b[36mchi2\u001b[39m\u001b[34m(X, y)\u001b[39m\n\u001b[32m    266\u001b[39m X = check_array(X, accept_sparse=\u001b[33m\"\u001b[39m\u001b[33mcsr\u001b[39m\u001b[33m\"\u001b[39m, dtype=(np.float64, np.float32))\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.any((X.data \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;28;01melse\u001b[39;00m X) < \u001b[32m0\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInput X must be non-negative.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    270\u001b[39m \u001b[38;5;66;03m# Use a sparse representation for Y by default to reduce memory usage when\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[38;5;66;03m# y has many unique classes.\u001b[39;00m\n\u001b[32m    272\u001b[39m Y = LabelBinarizer(sparse_output=\u001b[38;5;28;01mTrue\u001b[39;00m).fit_transform(y)\n",
      "\u001b[31mValueError\u001b[39m: Input X must be non-negative."
     ]
    }
   ],
   "source": [
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Feature Selection - Mutual Information\n",
    "mi = mutual_info_classif(X_train_scaled, y_train)\n",
    "mi_series = pd.Series(mi, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "# Feature Selection - Chi-Square\n",
    "X_chi2 = SelectKBest(score_func=chi2, k=20).fit(X_train_scaled, y_train)\n",
    "chi2_features = X.columns[X_chi2.get_support()]\n",
    "\n",
    "# Feature Selection - ANOVA F-test\n",
    "anova_selector = SelectKBest(score_func=f_classif, k=20)\n",
    "anova_selector.fit(X_train_scaled, y_train)\n",
    "anova_features = X.columns[anova_selector.get_support()]\n",
    "\n",
    "# Combine features from MI, Chi2, and ANOVA (intersection)\n",
    "selected_features = list(set(mi_series.head(20).index) & set(chi2_features) & set(anova_features))\n",
    "X_train_sel = X_train_scaled[:, [X.columns.get_loc(f) for f in selected_features]]\n",
    "X_test_sel = X_test_scaled[:, [X.columns.get_loc(f) for f in selected_features]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f5b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
